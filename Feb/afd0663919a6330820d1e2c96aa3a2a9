A worker who alleges reviewing graphic social media posts harmed his mental health can sue Facebook owner Meta, a Kenyan labour court says. Daniel Motaung claims he was paid about $2.20 (£1.80) per hour to review posts including beheadings and child abuse. He is also suing his then employer Sama, which Meta had contracted to review posts. Meta argued that the court had no jurisdiction because the company is not based in Kenya, Reuters reported. But the court disagreed and found that Meta and Sama were "proper parties" to the case. Meta declined to comment, but legal campaign group Foxglove expected it to appeal. In 2020, Meta paid $52m to settle a case brought by US content moderators over mental health issues which they said they developed on the job. The Kenyan case is supported by Foxglove, whose director Cori Crider said that a key point had been established: "Daniel's win today should send a message to Facebook, and by proxy, all of big tech in Africa. "Kenyan justice is equal to any tech giant, and the giants would do well to wake up and respect Kenyan people - and their law." Facebook employs thousands of moderators to review content flagged by users or artificial intelligence systems to see if it violates the platform's community standards, and to remove it if necessary. Mr Motaung said the first graphic video he saw was "a live video of someone being beheaded". He told the BBC in May 2022 that he suffers flashbacks where he imagines he is the victim. Mr Motaung, who says he has been diagnosed with post-traumatic stress disorder, believes that his co-workers also struggled with the content they had to view. "I would see people walking off the production floor to cry, you know, that type of thing," he said. Mr Motaung was recruited from South Africa to work for Sama in Nairobi, where much of the moderation for East and South Africa was handled. He claims the support given to moderators was inadequate. Sama has called his accusations "both disappointing and inaccurate", arguing that it provided all members of its workforce with a competitive wage, benefits, upward mobility, and a robust mental health and wellness programme. The firm has since ended its moderation work for Meta. Meta has previously declined to comment directly on the legal action, but has said it requires partners "to provide industry-leading pay, benefits and support". Campaigners say Monday's court ruling could have significance for other cases they are attempting to bring. In December, a case, also backed by Foxglove, was launched in Kenya, alleging Facebook's algorithm helped fuel the viral spread of hate and violence during Ethiopia's civil war. The case asks the court to require the creation of a $2bn fund for victims of hate on Facebook and changes to the platform's algorithm. Meta said that hate speech and incitement to violence were against the platform's rules, and says it invests heavily in moderation and tech to remove such content. It told the BBC: "We employ staff with local knowledge and expertise and continue to develop our capabilities to catch violating content in the most widely spoken languages in the country, including Amharic, Oromo, Somali and Tigrinya."